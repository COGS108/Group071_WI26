{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aditya Jadhav: Conceptualization, Data curation, Analysis, Writing – review & editing\n",
    "- Swayam Dani: Conceptualization, Methodology, Analysis, Visualization\n",
    "- Albert Bunyi: Data curation, Software, Analysis\n",
    "- Sean Yang: Background research, Visualization, Writing – original draft\n",
    "- Benjamin Balingit: Project administration, Methodology, Writing – original draft, Writing – review & editing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does tyre degradation within individual stints, measured as the difference between the fastest lap and the final lap of that stint, differ between Red Bull Racing drivers across circuits during the 2023 to 2025 regulation era, and how is that degradation related to the tyre age at the end of the stint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula 1 is a technologically advanced motorsport in which race performance is shaped not only by final results, but also by underlying factors such as tyre behavior, car setup, and track characteristics. Modern Formula 1 cars operate at the intersection of mechanical engineering, aerodynamics, and data-driven decision making. Over recent decades, advancements in engineering and data collection have made it possible to analyze race performance at a much finer granularity, including lap-by-lap pace and stint-level trends. Prior work has shown that these factors play a critical role in shaping race dynamics, particularly through tyre degradation and consistency of lap times over a race distance.\n",
    "\n",
    "Engineering-focused analyses of Formula 1 emphasize how regulatory stability and technical innovation influence performance trends over time. For example, a historical review of Formula 1 engineering development highlights how improvements in materials, aerodynamics, and power units have steadily increased both performance and data availability in the sport (Evolution of Formula One Motorsport, Academia.edu). This work demonstrates that as engineering systems become more sophisticated, performance differences increasingly emerge through operational factors such as tyre management and race strategy rather than raw speed alone. This context motivates a closer examination of lap-level performance metrics, which can reveal meaningful patterns beyond race outcomes.\n",
    "\n",
    "More focused prior work has examined tyre compounds and degradation directly. The official Formula 1 tyre guide explains how different compounds are designed to trade off grip and durability, with softer tyres providing higher initial performance at the cost of faster degradation (Formula1.com, Beginner's Guide to F1 Tyres). Complementing this practical perspective, recent academic research has modeled tyre degradation using lap-time data, demonstrating that degradation can be quantified as a systematic increase in lap times over a stint rather than random noise (arXiv:2512.00640). These studies show that tyre degradation and lap-time variability are measurable, interpretable phenomena. However, much of this work focuses on individual races or modeling approaches, rather than descriptive, multi-season analysis. Our project builds on these insights by examining lap-time degradation and driver consistency across multiple seasons (2021-2025), tyre compounds, and track types using publicly available race data.\n",
    "\n",
    "https://www.academia.edu/129272726/EVOLUTION_OF_FORMULA_ONE_F1_MOTORSPORTS_AND_ITS_TOP_NOTCH_ADVANCEMENT_IN_ENGINEERING_INNOVATIONS_ACROSS_THE_RACING_INDUSTRY \n",
    "\n",
    "https://www.formula1.com/en/latest/article/the-beginners-guide-to-formula-1-tyres.61SvF0Kfg29UR2SPhakDqd?utm_source \n",
    " \n",
    "https://arxiv.org/abs/2512.00640?utm_source "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that softer tyre compounds will show faster lap-time degradation than harder compounds across all tracks. We also expect that in lap-times will be shorter earlier in the stint when the tyres are fresher as compared to later in the stint when the tyres are older. We anticipate that the tyre degradation for both drivers of the Red Bull Racing team will be similar due to running the same car. Finally, we anticipate that driver consistency will vary by track type but remain relatively stable within the same driver across races."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name: OpenF1 dataset\n",
    "  - Link to the dataset: https://openf1.org/?python#csv-format\n",
    "    - There are multiple endpoints but we grabbed 3 endpoints and merged them: Drivers, Sessions, Stints\n",
    "    - Drivers: https://api.openf1.org/v1/drivers?&csv=true\n",
    "    - Sessions: https://api.openf1.org/v1/sessions?&csv=true\n",
    "    - Stints: https://api.openf1.org/v1/stints?&csv=true\n",
    "  - Number of observations: \n",
    "    - Final dataset observations = 76\n",
    "  - Number of variables: \n",
    "    - Final dataset variables = 10\n",
    "    - Total dataset variables = 28\n",
    "  - Description of the variables most relevant to this project\n",
    "    - compound = describes the tyre compound type during the given stint\n",
    "    - driver_number = the unique identifying number given to each driver\n",
    "    - lap_end = the last lap of the stint on one set of tyres\n",
    "    - lap_start = the first lap of the stint on one set of tyres\n",
    "    - meeting_key = the unique key that refers to the GrandPrix that the stint takes place in\n",
    "    - session_key = the unique key that refers to the kind of event going on during the stint (eg. practice, qualify, sprint, race)\n",
    "    - stint_number = the number for the current sequence of laps between pitstops that the driver is on\n",
    "    - tyre_age_at_start = the age of the tyres when they are put on in number of laps\n",
    "    - fastest_lap_duration = the duration of fastest lap in seconds of the stint\n",
    "    - lap_end_duration = the duration of the last lap of the stint in seconds\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "    - The data set does not take into account factors that may have an effect on the rate of tyre degredation such as track temperatures, how hard the driver is pushing, whether the driver is defending, if the driver is stuck in the dirty air of another car, or if the track in general has a higher amount of tyre wear due to its design. \n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "drivers = pd.read_csv(\"drivers.csv\")\n",
    "\n",
    "redbull_drivers = (\n",
    "    drivers[drivers[\"team_name\"].str.contains(\"Red Bull\", case=False, na=False)]\n",
    "    .drop(columns=[\"headshot_url\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "redbull_drivers.to_csv(\"redbull_drivers.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redbull_drivers = pd.read_csv(\"redbull_drivers.csv\")\n",
    "stints = pd.read_csv(\"stints.csv\")\n",
    "\n",
    "rb_numbers = redbull_drivers[\"driver_number\"].unique()\n",
    "rb_stints = stints[stints[\"driver_number\"].isin(rb_numbers)].reset_index(drop=True)\n",
    "\n",
    "rb_stints.to_csv(\"rb_stints.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_csv(\"sessions.csv\")\n",
    "rb_stints = pd.read_csv(\"rb_stints.csv\")\n",
    "\n",
    "rb_stints = rb_stints.merge(\n",
    "    sessions[[\"session_key\", \"year\", \"session_type\"]],\n",
    "    on=\"session_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "rb_stints = rb_stints[\n",
    "    (rb_stints[\"year\"].notna()) &\n",
    "    (rb_stints[\"year\"] != 2026) &\n",
    "    (rb_stints[\"session_type\"] == \"Race\")\n",
    "]\n",
    "\n",
    "rb_stints = rb_stints.drop(columns=[\"year\", \"session_type\"]).reset_index(drop=True)\n",
    "\n",
    "rb_stints.to_csv(\"rb_stints.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dates = pd.to_datetime([\n",
    "    \"2023-04-29\",\n",
    "    \"2023-07-29\",\n",
    "    \"2023-10-07\",\n",
    "    \"2023-10-21\",\n",
    "    \"2023-11-04\",\n",
    "    \"2024-04-20\",\n",
    "    \"2024-05-04\",\n",
    "    \"2024-06-29\",\n",
    "    \"2024-10-19\",\n",
    "    \"2024-11-02\",\n",
    "    \"2024-11-30\",\n",
    "    \"2025-03-22\",\n",
    "    \"2025-05-03\",\n",
    "    \"2025-07-26\",\n",
    "    \"2025-10-18\",\n",
    "    \"2025-11-08\",\n",
    "    \"2025-11-29\",\n",
    "]).date\n",
    "\n",
    "tmp = rb_stints.merge(\n",
    "    sessions[[\"session_key\", \"date_start\"]],\n",
    "    on=\"session_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "tmp[\"date_start\"] = pd.to_datetime(tmp[\"date_start\"], errors=\"coerce\").dt.date\n",
    "\n",
    "filtered = tmp[~tmp[\"date_start\"].isin(bad_dates)] \\\n",
    "            .drop(columns=[\"date_start\"]) \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "filtered.to_csv(\"rb_stints.csv\", index=False)\n",
    "\n",
    "print(\"Dimensions of the dataset:\")\n",
    "rb_stints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_stints = pd.read_csv(\"rb_stints.csv\")\n",
    "\n",
    "rb_stints[\"lap_start\"] = pd.to_numeric(rb_stints[\"lap_start\"], errors=\"coerce\")\n",
    "rb_stints[\"lap_end\"]   = pd.to_numeric(rb_stints[\"lap_end\"], errors=\"coerce\")\n",
    "\n",
    "rb_stints[\"lap_end_duration\"] = np.nan\n",
    "rb_stints[\"fastest_lap_duration\"] = np.nan\n",
    "\n",
    "groups = rb_stints.groupby([\"driver_number\", \"session_key\"]).groups\n",
    "\n",
    "for (driver_number, session_key), idx in groups.items():\n",
    "    driver_number = int(driver_number)\n",
    "    session_key = int(session_key)\n",
    "\n",
    "    url = f\"https://api.openf1.org/v1/laps?driver_number={driver_number}&session_key={session_key}\"\n",
    "    laps = pd.read_json(url)\n",
    "\n",
    "    if laps is None or laps.empty:\n",
    "        continue\n",
    "\n",
    "    laps[\"lap_number\"] = pd.to_numeric(laps[\"lap_number\"], errors=\"coerce\")\n",
    "    laps[\"lap_duration\"] = pd.to_numeric(laps[\"lap_duration\"], errors=\"coerce\")\n",
    "\n",
    "    for i in idx:\n",
    "        lap_start = rb_stints.at[i, \"lap_start\"]\n",
    "        lap_end = rb_stints.at[i, \"lap_end\"]\n",
    "\n",
    "        if pd.isna(lap_start) or pd.isna(lap_end):\n",
    "            continue\n",
    "\n",
    "        lap_start = int(lap_start)\n",
    "        lap_end = int(lap_end)\n",
    "\n",
    "        stint_laps = laps[(laps[\"lap_number\"] >= lap_start) & (laps[\"lap_number\"] <= lap_end)]\n",
    "        if stint_laps.empty:\n",
    "            continue\n",
    "\n",
    "        rb_stints.at[i, \"fastest_lap_duration\"] = stint_laps[\"lap_duration\"].min(skipna=True)\n",
    "\n",
    "        end_row = stint_laps[stint_laps[\"lap_number\"] == lap_end]\n",
    "        if not end_row.empty:\n",
    "            rb_stints.at[i, \"lap_end_duration\"] = end_row[\"lap_duration\"].iloc[0]\n",
    "\n",
    "rb_stints.to_csv(\"rb_stints.csv\", index=False)\n",
    "\n",
    "print(\"Updated rb_stints.csv\")\n",
    "print(\"Rows with missing lap_start or lap_end:\", rb_stints[\"lap_start\"].isna().sum() + rb_stints[\"lap_end\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_stints = rb_stints.dropna(subset=[\"lap_start\", \"lap_end\"]).reset_index(drop=True)\n",
    "\n",
    "rb_stints.to_csv(\"rb_stints.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics \n",
    "\n",
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "\n",
    "- put an X there if you've considered the item\n",
    "- IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "\n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section. You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\\\n",
    "<br>This project relies exclusively on publicly available Formula 1 race data, including lap times, tyre compounds, and circuit characteristics released by Formula 1, the FIA, and reputable third-party motorsport databases. No direct interaction with drivers, teams, or other individuals occurs, and no private or proprietary data are collected. Because the data are historical, observational, and already in the public domain, informed consent from individuals is not applicable in this context.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\\\n",
    "<br>We recognize that Formula 1 data collection may introduce biases related to race-specific events such as safety cars, red flags, weather changes, and retirements, which can disproportionately affect lap-time degradation measurements. Additionally, differences in track layouts and race lengths between street circuits and permanent circuits may lead to uneven representation across categories. To address these issues, we explicitly document data exclusions, normalize lap-time metrics where appropriate, and interpret results within the context of these known sources of bias rather than attributing outcomes solely to tyre compounds or driver performance.\n",
    "\n",
    "- **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "- **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    "<br>All our data comes from public databases<br>\n",
    "- **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "- **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "- **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "Data will only be stored for the duration of the course project. After the project is completed, local copies will be deleted and only final analysis outputs will remain in the project repository.\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\\\n",
    "<br>Our analysis focuses on quantitative performance metrics and does not capture qualitative factors such as driver feedback, team communications, or real-time strategic decision-making during races. These perspectives may influence tyre management and lap-time consistency but are not directly observable in the available data. To mitigate this limitation, we contextualize findings using existing Formula 1 regulations, race reports, and established motorsport knowledge, and we avoid making causal claims that would require access to these missing perspectives.\n",
    "\n",
    " - [x] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\\\n",
    "<br>The dataset may reflect imbalances in tyre compound usage, circuit types, and competitive conditions across the 2021–2025 regulation era. For example, certain tyre compounds may be used more frequently at specific circuits, and stronger teams may complete longer stints under favorable conditions. We address these biases by stratifying analyses by circuit type, controlling for stint length, and clearly stating assumptions and limitations where confounding variables cannot be fully removed.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\\\n",
    "<br>All visualizations and summary statistics are designed to accurately reflect the underlying race data without exaggerating trends or masking variability. Axis scales, aggregation choices, and comparisons between tyre compounds and circuit types are selected to avoid misleading interpretations. Where variability is high or sample sizes differ across circuits or seasons, this uncertainty is clearly communicated rather than smoothed over.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\\\n",
    "<br>The analysis does not involve private or sensitive information. All data used consist of publicly available lap times, tyre choices, and circuit classifications that are widely reported in Formula 1 coverage. No attempts are made to infer personal characteristics or private behavior beyond what is explicitly observable in race performance data.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\\\n",
    "<br>The full analytical process, including data sources, preprocessing steps, filtering criteria, and modeling choices, is documented to ensure reproducibility. This documentation allows results to be independently verified and enables future review if errors, biases, or alternative interpretations are identified.\n",
    "\n",
    "### D. Modeling\n",
    "- **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "- **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\\\n",
    "<br>We carefully select modeling metrics to align with the research question and avoid misleading optimization. Instead of relying on a single outcome measure, we analyze multiple complementary metrics, including lap-time degradation rates and measures of lap-time variability to capture driver consistency. This approach reduces the risk that conclusions are driven by artifacts of a single metric and allows for a more nuanced comparison across tyre compounds and circuit types.\n",
    "\n",
    "- **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\\\n",
    "<br>We clearly communicate the limitations of our modeling approach, including the observational nature of Formula 1 race data, the presence of unobserved confounding variables such as weather and strategic decisions, and simplifications involved in classifying circuits and tyre behavior. Model outputs are presented as descriptive or associative findings rather than causal claims, and conclusions are framed to avoid overgeneralization beyond the scope of the data.\n",
    "\n",
    "### E. Deployment\n",
    "- **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "- **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "- **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\\\n",
    "<br>Although this project is not deployed as a production system, we acknowledge the possibility that analytical results could be misinterpreted or taken out of context. For example, findings about lap-time degradation or driver consistency could be incorrectly attributed solely to driver skill rather than broader factors such as team resources, race strategy, or external conditions. To mitigate this risk, results are presented with clear explanations, appropriate caveats, and explicit statements about the observational and non-causal nature of the analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All team members will communicate regularly via Messages, GitHub issues, and biweekly zoom meetings to update one another on the current progress of their tasks.  \n",
    "- Expected tone: All team members will communicate with a friendly tone when bringing up issues, and will not personally target one another. For example: “ I am confused with X, could you explain further what you mean by that”  \n",
    "- Expectations around tasks: Tasks will just be assigned based on need, so there will not be any specialized roles.  \n",
    "- Expectations for struggling to deliver on time: Members will be expected to communicate if they are struggling to complete tasks on time, but if there are issues contacting anyone within the group, the group will try to take over the task within 2 days of the original deadline assigned.  \n",
    "- Each member will complete assigned tasks before agreed deadlines and notify the group early if delays arise.  \n",
    "- All analysis and writing will be reviewed by at least one other team member before submission.  \n",
    "- Disagreements will be resolved respectfully through discussion and reference to project goals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Extraction & Stint Structuring  \n",
    "**Feb 6 – Feb 14**\n",
    "\n",
    "Since our unit of analysis is the stint, this phase focuses on building a clean stint-level dataset.\n",
    "\n",
    "### Tasks:\n",
    "- Filter dataset to Red Bull Racing drivers (2023–2025)  \n",
    "- Identify and separate individual stints  \n",
    "- For each stint, compute:\n",
    "  - Fastest lap  \n",
    "  - Final lap  \n",
    "  - Degradation gap (final lap time minus fastest lap time)  \n",
    "  - Tyre age on the final lap  \n",
    "- Validate data consistency and remove abnormal stints  \n",
    "\n",
    "### Deliverable:\n",
    "Clean, structured stint-level dataset ready for analysis  \n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: Exploratory Data Analysis (Checkpoint #2)  \n",
    "**Feb 15 – Feb 23**\n",
    "\n",
    "### Tasks:\n",
    "- Examine distributions of:\n",
    "  - Stint lengths  \n",
    "  - Tyre age at final lap  \n",
    "  - Degradation gaps  \n",
    "- Identify outliers and unusual patterns  \n",
    "- Generate initial visualizations:\n",
    "  - Fastest lap vs final lap scatter plot  \n",
    "  - Distribution of degradation gaps  \n",
    "  - Degradation vs tyre age  \n",
    "\n",
    "### Deliverable:\n",
    "EDA notebook with interpretations ready for submission  \n",
    "\n",
    "---\n",
    "\n",
    "## Phase 3: Intra-Team & Circuit-Level Analysis  \n",
    "**Feb 24 – Mar 3**\n",
    "\n",
    "### Tasks:\n",
    "- Compare degradation distributions between Red Bull drivers  \n",
    "- Analyze degradation across circuits  \n",
    "- Evaluate the relationship between tyre age and degradation  \n",
    "- Assess variation across seasons (2023, 2024, 2025)  \n",
    "- Finalize summary statistics and key comparisons  \n",
    "\n",
    "### Deliverable:\n",
    "Core analysis completed and metrics finalized  \n",
    "\n",
    "---\n",
    "\n",
    "## Phase 4: Visualization & Writing  \n",
    "**Mar 4 – Mar 9**\n",
    "\n",
    "### Tasks:\n",
    "- Create polished final visualizations:\n",
    "  - Driver comparison plots  \n",
    "  - Degradation vs tyre age  \n",
    "  - Circuit-level variation  \n",
    "- Write:\n",
    "  - Background & Prior Work  \n",
    "  - Methods section (clearly stating stint as the unit of analysis)  \n",
    "  - Results interpretation  \n",
    "  - Ethics and bias discussion  \n",
    "  - Limitations  \n",
    "\n",
    "### Deliverable:\n",
    "Complete draft of final project notebook  \n",
    "\n",
    "---\n",
    "\n",
    "## Phase 5: Final Review & Submission  \n",
    "**Mar 10 – Mar 13**\n",
    "\n",
    "### Tasks:\n",
    "- Proofread and refine writing  \n",
    "- Ensure the research question is clearly answered  \n",
    "- Confirm all plots are labeled and interpreted  \n",
    "- Restart kernel and run the notebook end-to-end  \n",
    "- Final GitHub commit and submission  \n",
    "- Complete individual peer evaluations  \n",
    "\n",
    "**Deliverable:** Final Project Submission (March 13) (3/20 is the actual deadline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
